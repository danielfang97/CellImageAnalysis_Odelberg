{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielfang97/CellImageAnalysis_Odelberg/blob/main/NLP_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WL0op5CRrh88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKnvHLcps8F6",
        "outputId": "5f82d466-1624-48a3-dda3-39cb99b9bcb1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/content/drive/MyDrive/Subtask_1_train.json')\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "id": "HfUAEZsAtGZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3490558-152f-4bbb-8d79-b8c6f53c54c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      conversation_ID                                       conversation  \\\n",
            "0                   1  [{'utterance_ID': 1, 'text': 'Alright , so I a...   \n",
            "1                   2  [{'utterance_ID': 1, 'text': 'I do not want to...   \n",
            "2                   3  [{'utterance_ID': 1, 'text': 'Oh my God !', 's...   \n",
            "3                   4  [{'utterance_ID': 1, 'text': 'Barry , I am sor...   \n",
            "4                   5  [{'utterance_ID': 1, 'text': 'Oh , look , wish...   \n",
            "...               ...                                                ...   \n",
            "1369             1346  [{'utterance_ID': 1, 'text': 'Hey Ross , this ...   \n",
            "1370             1357  [{'utterance_ID': 1, 'text': 'Yeah , but what ...   \n",
            "1371             1360  [{'utterance_ID': 1, 'text': 'Wow ! Fortunatel...   \n",
            "1372             1371  [{'utterance_ID': 1, 'text': 'Ooh ! I am sorry...   \n",
            "1373             1374  [{'utterance_ID': 1, 'text': 'I never sucked ,...   \n",
            "\n",
            "                                    emotion-cause_pairs  \n",
            "0     [[3_surprise, 1_I realize I am totally naked ....  \n",
            "1     [[1_sadness, 1_I do not want to be single], [3...  \n",
            "2     [[3_sadness, 3_I should have caught on when sh...  \n",
            "3                                                    []  \n",
            "4     [[1_joy, 3_I am gonna go get one of those job ...  \n",
            "...                                                 ...  \n",
            "1369                                                 []  \n",
            "1370  [[1_fear, 1_if there is a reason why we can no...  \n",
            "1371  [[2_surprise, 2_My uterus is an inhospitable e...  \n",
            "1372  [[1_joy, 1_that may have missed the table !], ...  \n",
            "1373  [[2_surprise, 1_I never sucked , I actually di...  \n",
            "\n",
            "[1374 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daniel's code"
      ],
      "metadata": {
        "id": "lc4tKvlyISjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recurrent Neural Network (RNN) using Long Short-Term Memory (LSTM)"
      ],
      "metadata": {
        "id": "9UPu-H3oIYdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Turn the data into a readable format."
      ],
      "metadata": {
        "id": "-uPM65_5J9EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = []\n",
        "emotions = []\n",
        "speakers = []\n",
        "\n",
        "for conversation in df['conversation']:\n",
        "    utterance_texts = [utterance['text'] for utterance in conversation]\n",
        "    utterance_emotions = [utterance['emotion'] for utterance in conversation]  # Assuming each utterance has an 'emotion' key\n",
        "    utterance_speakers = [utterance['speaker'] for utterance in conversation]\n",
        "    texts.append(utterance_texts)\n",
        "    emotions.append(utterance_emotions)\n",
        "    speakers.append(utterance_speakers)"
      ],
      "metadata": {
        "id": "5mhxHhWaJ61S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import itertools\n",
        "\n",
        "# Assuming 'texts', 'emotions', and 'speakers' are nested lists\n",
        "\n",
        "# Flatten the nested lists for emotion and speaker labels\n",
        "flat_emotions = list(itertools.chain(*emotions))  # Flatten the nested emotion list\n",
        "flat_speakers = list(itertools.chain(*speakers))  # Flatten the nested speakers list\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts([text for sublist in texts for text in sublist])  # Flatten and fit\n",
        "sequences = [tokenizer.texts_to_sequences(sublist) for sublist in texts]\n",
        "padded_sequences = [pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type) for seq in sequences]\n",
        "\n",
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "emotion_labels = label_encoder.fit_transform(flat_emotions)\n",
        "speaker_labels = label_encoder.fit_transform(flat_speakers)\n",
        "\n",
        "# Preparing the data for the model\n",
        "# Your input (X) is 'padded_sequences'\n",
        "# Your emotion labels (y_emotion) are 'emotion_labels'\n",
        "# Your speaker labels (y_speaker) are 'speaker_labels'"
      ],
      "metadata": {
        "id": "MXisRWNYIhNQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = 10000   # Replace with your vocabulary size\n",
        "embedding_dim = 256  # Dimension of embedding layer\n",
        "lstm_units = 128     # Number of units in LSTM layer\n",
        "max_length = 50      # Maximum length of input sequences\n",
        "padding_type = 'post'\n",
        "trunc_type = 'post'\n",
        "\n",
        "# Pad sequences for uniformity\n",
        "X_padded = pad_sequences(X, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "# Building the model\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(lstm_units)),\n",
        "    Dense(24, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Change the number of units & activation based on your specific task\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy',  # Change the loss function based on your specific task\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_padded, y, epochs=10, validation_split=0.2)  # Adjust epochs, validation split as needed\n"
      ],
      "metadata": {
        "id": "DXa4EKTXPWcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}